{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IN43qsANwiG"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PolitoVandal/project-sim2real-delli-modi-necerini_project/blob/main/project_RL.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IujtCtCqoAFI"
      },
      "source": [
        "# Before starting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXo_JtTlyYvr"
      },
      "source": [
        "## Clone GitHub repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jROBwAj8ybHg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('/content/project-sim2real-delli-modi-necerini_project'):\n",
        "  !git clone -b main https://github.com/RonPlusSign/RobotLearningLabs.git\n",
        "\n",
        "# Rename the folder\n",
        "!mv /content/RobotLearningLabs /content/sim2real\n",
        "!cd /content/sim2real && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AJhGXFRyTa2"
      },
      "source": [
        "## Download libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW6XT0jSJI8e"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "\n",
        "!pip install gym\n",
        "!pip install free-mujoco-py\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install shimmy\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "from sim2real.env.custom_hopper import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrXPbdqkkQe-"
      },
      "source": [
        "# Lab 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wOHt7hlbRjL"
      },
      "source": [
        "## PPO Training on Custom Hopper environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXWsEj3ekfSp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings during training\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Create the 'models' directory if it doesn't exist\n",
        "if not os.path.exists('sim2real/models'):\n",
        "    os.makedirs('sim2real/models')\n",
        "\n",
        "# Create the 'plots' directory if it doesn't exist\n",
        "if not os.path.exists('sim2real/plots'):\n",
        "    os.makedirs('sim2real/plots')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq6oIFEi71u_"
      },
      "source": [
        "### Source training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6pvfXd1RbU_N",
        "outputId": "e14421df-3deb-4316-8e52-1a2ca7eb0173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training time: 844.9242844581604\n",
            "Test reward (avg +/- std): (610.2515685785082 +/- 173.88491636395824) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 1 - 500k timesteps\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --total_timesteps=500000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p8sIUUpJ76gO",
        "outputId": "cff7ab3a-33bb-4329-f497-185607563e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training time: 8449.11818575859\n",
            "Test reward (avg +/- std): (1544.6508584914657 +/- 304.80255938674514) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 2 - 5M timesteps\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --total_timesteps=5000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zyN3B5r0EGWD",
        "outputId": "765064e0-8319-45a1-c3fd-daab268300ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training time: 962.2152454853058\n",
            "Test reward (avg +/- std): (619.0097090676186 +/- 205.1345712081536) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 3 (test LINEAR LR schedule - 500k timesteps)\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --total_timesteps=500000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MSnYXYFlJ5BF",
        "outputId": "8f6acf3e-f4ef-4abe-c4c3-7602740592dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training time: 991.3553211688995\n",
            "Test reward (avg +/- std): (921.7609026647743 +/- 355.72170703486603) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 4 (test EXPONENTIAL LR schedule - 500k timesteps)\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --total_timesteps=500000 --lr_schedule='exponential'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pjh2r9RK4dbT",
        "outputId": "c3a98269-a077-44aa-dad0-d18c85770c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training time: 3698.463559150696\n",
            "Test reward (avg +/- std): (1226.324881008564 +/- 411.15862480353917) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 5 (test EXPONENTIAL LR schedule - 2M timesteps)\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --total_timesteps=2000000 --lr_schedule='exponential' --model_id=\"source_5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qCmlG1NdZGZ"
      },
      "source": [
        "### Target training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Xygw-iRK4O5i",
        "outputId": "9b2b3188-66a1-412f-ae6d-3c1037548734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training time: 844.8844656944275\n",
            "Test reward (avg +/- std): (651.8186004186551 +/- 87.96238364152971) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 1\n",
        "!cd sim2real; python train.py --env=CustomHopper-target-v0 --total_timesteps=500000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uvSGU-8681Bg",
        "outputId": "d38546fe-d21b-40ec-897d-0f5406b0ae04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training time: 8560.386691331863\n",
            "Test reward (avg +/- std): (1243.2276279905634 +/- 494.55743941516295) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 2\n",
        "!cd sim2real; python train.py --env=CustomHopper-target-v0 --total_timesteps=5000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "63-W-H9-mVWT",
        "outputId": "7ebdec3a-e39c-4788-9089-d681f572e58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training time: 3662.456332921982\n",
            "Test reward (avg +/- std): (1580.8654151466749 +/- 288.70644216599095) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training 3\n",
        "!cd sim2real; python train.py --env=CustomHopper-target-v0 --model_id=\"target_3\" --total_timesteps=2000000 --lr_schedule='exponential'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EmLmhNtwNG"
      },
      "source": [
        " ### Testing the trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8w4JKM7wtz2E",
        "outputId": "b2617e53-eee1-4adc-de1c-87a14e5b1fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- source_1 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_source_to_source_test_model_source_1.mp4\n",
            "Test reward (avg +/- std): (1202.062789947555 +/- 324.21626837181344) - Num episodes: 1000\n",
            "\n",
            "----- source_2 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_source_to_source_test_model_source_2.mp4\n",
            "Test reward (avg +/- std): (1534.4865589557387 +/- 266.00493908574356) - Num episodes: 1000\n",
            "\n",
            "----- source_3 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_source_to_source_test_model_source_3.mp4\n",
            "Test reward (avg +/- std): (1194.6528785591793 +/- 237.25899598913225) - Num episodes: 1000\n",
            "\n",
            "----- source_4 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_source_to_source_test_model_source_4.mp4\n",
            "Test reward (avg +/- std): (1263.3068066547035 +/- 215.9125573593509) - Num episodes: 1000\n",
            "\n",
            "----- source_5 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_source_to_source_test_model_source_5.mp4\n",
            "Test reward (avg +/- std): (1466.7613401775973 +/- 310.1469499175551) - Num episodes: 1000\n",
            "\n",
            "----- target_1 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_target_to_target_test_model_target_1.mp4\n",
            "Test reward (avg +/- std): (661.912386168446 +/- 44.420249570852) - Num episodes: 1000\n",
            "\n",
            "----- target_2 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_target_to_target_test_model_target_2.mp4\n",
            "Test reward (avg +/- std): (1507.546083675303 +/- 380.78143268474844) - Num episodes: 1000\n",
            "\n",
            "----- target_3 -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_target_to_target_test_model_target_3.mp4\n",
            "Test reward (avg +/- std): (1466.7613401775973 +/- 310.1469499175551) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# Test the 5 models trained on SOURCE and 3 models trained on TARGET\n",
        "models = ['source_1', 'source_2', 'source_3', 'source_4', 'source_5', 'target_1', 'target_2', 'target_3']\n",
        "\n",
        "for model_id in models:\n",
        "  print(f\"\\n----- {model_id} -----\")\n",
        "  env_name = model_id.split('_')[0]\n",
        "  !cd sim2real; python train.py --test=models/PPO_model_{model_id} --env=CustomHopper-{env_name}-v0 --test_episodes=1000 --test_rendering --video_name=hopper_{env_name}_to_{env_name}_test_model_{model_id}.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiZMgsHnPXLK"
      },
      "source": [
        "### Test baseline models (source_5 and target_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKA-Kf0kPdBQ",
        "outputId": "51fc571e-93d1-4ace-a3f4-34fee0d758fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_source_to_source_test_model_source_5.mp4\n",
            "Test reward (avg +/- std): (1471.4690957737257 +/- 311.72029948778004) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_source_to_target_test_model_source_5.mp4\n",
            "Test reward (avg +/- std): (1495.0460916799027 +/- 253.6159042769558) - Num episodes: 1000\n",
            "\n",
            "----- TARGET ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_target_to_target_test_model_target_3.mp4\n",
            "Test reward (avg +/- std): (1696.4763196462113 +/- 93.63530584994669) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_source_5\" --env=CustomHopper-source-v0 --test_episodes=1000 --test_rendering --video_name=\"hopper_source_to_source_test_model_source_5.mp4\"\n",
        "\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_source_5\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"hopper_source_to_target_test_model_source_5.mp4\"\n",
        "\n",
        "print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_target_3\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"hopper_target_to_target_test_model_target_3.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv3ioxzC2kUi"
      },
      "source": [
        "## Grid search of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpTQttKG2nEE",
        "outputId": "3fe5b7ff-1ef9-41b7-ba5d-2263df82206e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------- TRAINING ON SOURCE ENVIRONMENT --------\n",
            "\n",
            "Starting grid search...\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 332.0210466647148\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 338.30170162439344\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 333.1047730827332\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 315.7885083663464\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 367.29987224698067\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 183.92171713232995\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 375.8082304239273\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 318.2318933725357\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 279.38823335886\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 377.2446205842495\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 361.8412185502052\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 308.5579565668106\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 375.72691783428195\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 334.8987160813808\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 305.94564486980437\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 622.1725521671772\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 190.93068699002265\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 291.41815992951393\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 435.22177042007445\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 388.5639603459835\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 304.4200087583065\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 328.69603119015693\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 220.54557418107987\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 223.34473692655564\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 301.964225114584\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 344.3131103348732\n",
            "Training with n_epochs=5, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 365.7304089665413\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 348.8758598530292\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 393.3628531074524\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 388.4766134631634\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 428.372281563282\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 343.5743098604679\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 318.43876412272454\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 378.2033540201187\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 332.19024080753326\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 366.297019174099\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 385.62862759470937\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 340.06495158672334\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 514.7843967998028\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 382.216216057539\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 428.48253360390663\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 441.35858315467834\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 393.1560013628006\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 393.41187385201454\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 303.423252620101\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 181.36206632614136\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 382.3719919717312\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 367.59356535315516\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 602.9313436949253\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 283.7135656452179\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 321.99758924007415\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 223.0721909570694\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 307.2278830766678\n",
            "Training with n_epochs=5, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 325.14736141085626\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 398.989153727293\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 382.0603217089176\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 371.8816826981306\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 336.9343211555481\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 278.8044179868698\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 276.9461367815733\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 390.27530788064\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 375.96593508839607\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 164.379238550663\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 399.7235306024551\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 411.46036464333537\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 342.5557121741772\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 363.8242163181305\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 360.3849353003502\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 481.2224121785164\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 381.7930268841982\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 246.26486282467843\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 547.7537430608272\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 359.5297055530548\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 399.2072977399826\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 560.8911496460438\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 498.2689363282919\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 317.3017358219624\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 335.9403981947899\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 329.70847451508047\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 1006.5759580409526\n",
            "Training with n_epochs=5, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 225.14820392608644\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 346.39274061083796\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 353.4899620437622\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 358.587205080986\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 351.6126259231567\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 379.8270166170597\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 377.2415342915058\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 384.7444983136654\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 374.0194467127323\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 370.15875497221947\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 431.1346298921108\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 371.9953097212315\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 374.5719294381142\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 384.0288614213467\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 468.12752479672434\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 290.33466114640237\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 672.6762292683125\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 383.8065099155903\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 322.17146638154986\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 355.9481563782692\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 394.8676422178745\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 387.4445950603485\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 471.26497133016585\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 224.5090256524086\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 333.156627920866\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 284.28771872878076\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 208.51918842196466\n",
            "Training with n_epochs=10, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 351.63358179807665\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 355.6589933419228\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 359.007928763628\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 349.42396001815797\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 308.9759763073921\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 330.94911831378937\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 330.139648655653\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 307.85763043045995\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 358.23313543319705\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 436.7580442249775\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 342.87935558080676\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 372.7671700632572\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 461.14692060232164\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 353.91702805757524\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 394.572711186409\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 398.76805461645125\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 406.38605188727377\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 355.40820228099824\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 398.24191405296324\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 541.3245076954365\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 477.6715900504589\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 414.33180577278137\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 363.52402628183364\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 772.1921160829068\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 1063.0805686235428\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 320.44177625179293\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 300.3274969291687\n",
            "Training with n_epochs=10, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 967.459112457037\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 344.80171887636186\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 295.0864629995823\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 517.5928646981716\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 307.75653381943704\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 393.6844548916817\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 334.5275540781021\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 367.81076807141307\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 282.9162799346447\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 465.2582879817486\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 345.37508972227573\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 340.2670153129101\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 430.7117803871632\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 386.6498807060718\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 414.4723651230335\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 383.08940607070923\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 755.7317327344417\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 635.9626500344276\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 496.36056278586386\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 385.6445137512684\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 351.67438625335694\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 431.47617659926414\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 647.7942603111267\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 810.4693379807472\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 332.1934925723076\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 223.4034586071968\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 1038.7594701361656\n",
            "Training with n_epochs=10, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 235.57028549671173\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 347.107480506897\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 332.75333013534544\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 338.30418251514436\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 432.12183973908424\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 365.2711278951168\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 371.5458020925522\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 358.48439661860465\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 376.01803497195243\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 346.86178996801374\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 332.3300949907303\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 337.56412446022034\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 329.4953531742096\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 395.1402267420292\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 575.9907317340374\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 386.79060005664826\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 329.62747569441797\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 310.10443168759343\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 309.01325717449186\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 331.01489631175997\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 374.6756824159622\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 356.7013199150562\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 559.6723620295525\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 537.4355707097053\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 354.94000627756117\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 322.3224729204178\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 257.3170688337088\n",
            "Training with n_epochs=20, clip_range=0.1, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 224.25962050199507\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 374.55876695632935\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 341.52473354816436\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 344.31436197280885\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 380.54266226172444\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 363.4257883620262\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 334.20724330067634\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 449.33721831083295\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 325.0122709822655\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 345.6548518979549\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 338.7915208137035\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 334.08632769465447\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 347.9439650011063\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 311.5056290256977\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 351.63169079065324\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 377.9560611557961\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 237.76297233343124\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 375.60248911440374\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 368.92788687169553\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 325.4636043417454\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 346.7102500987053\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 338.0712111532688\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 1131.4913163626195\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 635.4936446404457\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 445.6045650756359\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 341.2917765212059\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 203.7689554786682\n",
            "Training with n_epochs=20, clip_range=0.2, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 896.4859563279152\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 326.83063379883765\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 379.5483924853802\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 355.0007449448109\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 343.5382424265146\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 321.499549574852\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 324.8653148603439\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 346.6855973851681\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 344.742009768486\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.9, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 411.8167303192615\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 467.01677045106885\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 596.3078754115105\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 338.6995864915848\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 424.1505892431736\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 451.59507528841493\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 508.03553573966025\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 341.90604658842085\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 342.8730062055588\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.95, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 353.60825886130334\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=32\n",
            "   => Mean reward: 417.24054497599604\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=64\n",
            "   => Mean reward: 361.8965859997272\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.95, batch_size=128\n",
            "   => Mean reward: 351.04216147065165\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=32\n",
            "   => Mean reward: 401.31771882534025\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=64\n",
            "   => Mean reward: 362.51473833084106\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.99, batch_size=128\n",
            "   => Mean reward: 451.15856567025185\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=32\n",
            "   => Mean reward: 463.3957636713982\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=64\n",
            "   => Mean reward: 348.3038368797302\n",
            "Training with n_epochs=20, clip_range=0.3, gae_lambda=0.99, gamma=0.999, batch_size=128\n",
            "   => Mean reward: 1461.237786039114\n",
            "\n",
            "Grid search finished. Best reward: 1461.237786039114 with params: {'n_epochs': 20, 'clip_range': 0.3, 'gae_lambda': 0.99, 'gamma': 0.999, 'batch_size': 128}\n"
          ]
        }
      ],
      "source": [
        "# Grid search with 100k steps for each model\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --total_timesteps=100000 --grid_search --verbose=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bfz0z3W2XnS-",
        "outputId": "bff87453-c67a-4e8e-d130-d25574da7ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1103.78 +/- 211.34\n",
            "Episode length: 378.80 +/- 99.57\n",
            "Eval num_timesteps=1700000, episode_reward=1062.21 +/- 391.01\n",
            "Episode length: 427.60 +/- 144.80\n",
            "Eval num_timesteps=1800000, episode_reward=1044.69 +/- 266.07\n",
            "Episode length: 412.20 +/- 111.15\n",
            "Eval num_timesteps=1900000, episode_reward=960.97 +/- 69.81\n",
            "Episode length: 418.80 +/- 28.53\n",
            "Eval num_timesteps=2000000, episode_reward=1264.52 +/- 160.34\n",
            "Episode length: 460.40 +/- 79.20\n",
            "Training time: 4237.585305452347\n",
            "Test reward (avg +/- std): (1077.0278314890043 +/- 329.15435129331354) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Long training (2M timesteps) with best parameters from grid search\n",
        "#Grid search finished. Best reward: 1461.237786039114 with params: {'n_epochs': 20, 'clip_range': 0.3, 'gae_lambda': 0.99, 'gamma': 0.999, 'batch_size': 128}\n",
        "\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --model_id=\"best_source\" --total_timesteps=2000000 --n_epochs=20 --clip_range=0.3 --gae_lambda=0.99 --gamma=0.999 --batch_size=128 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-v7V3pR9zzKi",
        "outputId": "784463e9-dd6d-43d3-c6af-bbf545f36b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1217.10 +/- 105.70\n",
            "Episode length: 483.20 +/- 23.22\n",
            "Eval num_timesteps=1700000, episode_reward=1098.67 +/- 46.55\n",
            "Episode length: 500.00 +/- 0.00\n",
            "Eval num_timesteps=1800000, episode_reward=1135.02 +/- 21.49\n",
            "Episode length: 500.00 +/- 0.00\n",
            "Eval num_timesteps=1900000, episode_reward=1179.07 +/- 27.77\n",
            "Episode length: 500.00 +/- 0.00\n",
            "Eval num_timesteps=2000000, episode_reward=1140.78 +/- 211.08\n",
            "Episode length: 456.20 +/- 87.60\n",
            "Training time: 4240.038843870163\n",
            "Test reward (avg +/- std): (1155.7295022985215 +/- 215.3178084996668) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# long training with best parameters\n",
        "#Grid search finished. Best reward: 1461.237786039114 with params: {'n_epochs': 20, 'clip_range': 0.3, 'gae_lambda': 0.99, 'gamma': 0.999, 'batch_size': 128}\n",
        "\n",
        "!cd sim2real; python train.py --env=CustomHopper-target-v0 --model_id=\"best_target\" --total_timesteps=2000000 --n_epochs=20 --clip_range=0.3 --gae_lambda=0.99 --gamma=0.999 --batch_size=128 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yMQdTsNg20e"
      },
      "source": [
        "## Domain Randomization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq5XgPYlg5uq"
      },
      "source": [
        "### Train model using randomization with uniform distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IXZ4hXEIg5AL",
        "outputId": "23d234f9-6c64-4fbb-b81e-62557226551b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1375.79 +/- 237.27\n",
            "Episode length: 403.20 +/- 79.63\n",
            "Eval num_timesteps=1700000, episode_reward=1655.96 +/- 160.54\n",
            "Episode length: 458.00 +/- 51.54\n",
            "Eval num_timesteps=1800000, episode_reward=1766.72 +/- 25.05\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1900000, episode_reward=1665.03 +/- 107.70\n",
            "Episode length: 478.00 +/- 44.00\n",
            "Eval num_timesteps=2000000, episode_reward=1110.96 +/- 99.67\n",
            "Episode length: 306.00 +/- 18.25\n",
            "Training time: 3481.914088487625\n",
            "Test reward (avg +/- std): (1457.7240589302537 +/- 355.89458405554495) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Uniform Domain Randomization (2M timesteps)\n",
        "!cd sim2real; python train.py --model_id=\"source_randomized_uniform_1\" --domain_rand=uniform --env=CustomHopper-source-v0 --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP7ak5EVhjri"
      },
      "source": [
        "### Train model using randomization with truncated normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uUf7w6cWhtrg",
        "outputId": "ea40c011-cd50-4c92-d7d7-bb795bad2bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=869.20 +/- 434.94\n",
            "Episode length: 257.00 +/- 114.59\n",
            "Eval num_timesteps=1700000, episode_reward=1326.87 +/- 328.24\n",
            "Episode length: 394.20 +/- 98.88\n",
            "Eval num_timesteps=1800000, episode_reward=1194.24 +/- 267.77\n",
            "Episode length: 349.60 +/- 86.34\n",
            "Eval num_timesteps=1900000, episode_reward=1568.43 +/- 172.29\n",
            "Episode length: 452.80 +/- 49.77\n",
            "Eval num_timesteps=2000000, episode_reward=1419.25 +/- 155.85\n",
            "Episode length: 411.60 +/- 44.10\n",
            "Training time: 3603.520294904709\n",
            "Test reward (avg +/- std): (1338.7608543905403 +/- 283.3275855214356) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Truncated Normal Domain Randomization (2M timesteps)\n",
        "!cd sim2real; python train.py --model_id=\"source_randomized_gaussian_1\" --domain_rand=gaussian --env=CustomHopper-source-v0 --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCHSpfsrIx84"
      },
      "source": [
        "### Test randomized models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_XAvHsUHqyC",
        "outputId": "b574d592-2e5c-40ef-a043-33201e6b7483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "\n",
            "--- WORKING ON SOURCE ENVIRONMENT ---\n",
            "\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_randomized_uniform_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1719.8699071218934 +/- 8.322910960511003) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "\n",
            "--- WORKING ON TARGET ENVIRONMENT ---\n",
            "\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_randomized_uniform_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1721.6464029399863 +/- 9.576295941109272) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# Test PPO_model_source_randomized_uniform_1 (UDR)\n",
        "\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_source_randomized_uniform_1\" --env=CustomHopper-source-v0 --test_episodes=1000 --test_rendering --video_name=\"hopper_randomized_uniform_source_to_source_test_video.mp4\"\n",
        "\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_source_randomized_uniform_1\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"hopper_randomized_uniform_source_to_target_test_video.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH2_vAylIVEd",
        "outputId": "c317e6e4-78c7-41fe-9111-251caa814ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "\n",
            "--- WORKING ON SOURCE ENVIRONMENT ---\n",
            "\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_randomized_gaussian_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1286.190773661852 +/- 366.4682855154399) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "\n",
            "--- WORKING ON TARGET ENVIRONMENT ---\n",
            "\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/hopper_randomized_gaussian_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1351.565737943985 +/- 133.70407167964783) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# Test PPO_model_source_randomized_gaussian_1 (TNR)\n",
        "\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_source_randomized_gaussian_1\" --env=CustomHopper-source-v0 --test_episodes=1000 --test_rendering --video_name=\"hopper_randomized_gaussian_source_to_source_test_video.mp4\"\n",
        "\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_source_randomized_gaussian_1\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"hopper_randomized_gaussian_source_to_target_test_video.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p61ox6xZkKGr"
      },
      "source": [
        "# Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtY4QYceetPR"
      },
      "source": [
        "## 1. Learning rate schedule (Hopper)\n",
        "\n",
        "Train and test different learning rate schedules, in order to find the most appropriate one.\n",
        "\n",
        "Models trained: CustomHopper with source environment, and torso weight shift of -1kg with respect to target environment.\n",
        "\n",
        "Initial Learning Rate: 0.0003\n",
        "\n",
        "Tested schedules:\n",
        "- Constant (keep 0.0003 for all training)\n",
        "- Linear (lower the LR linearly at each episode, from initial to 0)\n",
        "- Exponential (LR decreases drastically only at the end of training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqUL7L6h7tmi"
      },
      "source": [
        "#### Training with 3 different LR schedules: constant, linear, exponential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FalDHl_2RjH",
        "outputId": "5195e49f-7573-4346-9cc4-ea286e92347d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1673.45 +/- 6.51\n",
            "Episode length: 500.00 +/- 0.00\n",
            "Eval num_timesteps=1700000, episode_reward=1589.74 +/- 243.96\n",
            "Episode length: 460.40 +/- 79.20\n",
            "Eval num_timesteps=1800000, episode_reward=1716.16 +/- 9.27\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1900000, episode_reward=1692.53 +/- 108.59\n",
            "Episode length: 474.60 +/- 33.54\n",
            "Eval num_timesteps=2000000, episode_reward=1718.50 +/- 17.79\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Training time: 3634.022445201874\n",
            "Test reward (avg +/- std): (1521.829145932036 +/- 267.6017010840901) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Constant Learning Rate (default: 0.0003)\n",
        "!cd sim2real && python train.py --model_id=\"customHopper_source_lr_schedule_constant\" --env=CustomHopper-source-v0 --total_timesteps=2000000 --lr_schedule=\"constant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0VxZkPw2bT4",
        "outputId": "de4ccad6-4700-4ca4-fa00-66be2f1c36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1753.37 +/- 3.66\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1700000, episode_reward=1760.51 +/- 24.54\n",
            "Episode length: 491.00 +/- 18.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1800000, episode_reward=1789.12 +/- 104.57\n",
            "Episode length: 464.00 +/- 25.85\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1900000, episode_reward=1770.97 +/- 38.11\n",
            "Episode length: 485.00 +/- 21.45\n",
            "Eval num_timesteps=2000000, episode_reward=1805.28 +/- 73.07\n",
            "Episode length: 489.40 +/- 21.20\n",
            "New best mean reward!\n",
            "Training time: 3638.6890399456024\n",
            "Test reward (avg +/- std): (1433.9130158764106 +/- 394.01911175889694) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Linear schedule for Learning Rate (initial: 0.0003)\n",
        "!cd sim2real && python train.py --model_id=\"customHopper_source_lr_schedule_linear\" --env=CustomHopper-source-v0 --total_timesteps=2000000 --lr_schedule=\"linear\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3BeOeEy2ex0",
        "outputId": "7c37774c-68aa-42c3-eb08-e7effcf8c772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1311.39 +/- 152.52\n",
            "Episode length: 388.60 +/- 56.30\n",
            "Eval num_timesteps=1700000, episode_reward=1594.51 +/- 6.32\n",
            "Episode length: 500.00 +/- 0.00\n",
            "Eval num_timesteps=1800000, episode_reward=1199.89 +/- 211.93\n",
            "Episode length: 357.20 +/- 73.43\n",
            "Eval num_timesteps=1900000, episode_reward=1623.56 +/- 5.66\n",
            "Episode length: 493.40 +/- 13.20\n",
            "Eval num_timesteps=2000000, episode_reward=1231.96 +/- 564.41\n",
            "Episode length: 369.00 +/- 158.17\n",
            "Training time: 3940.0959384441376\n",
            "Test reward (avg +/- std): (1079.0912841794334 +/- 498.22934691469777) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Exponential schedule for Learning Rate (initial: 0.0003, decay rate: 0.1)\n",
        "!cd sim2real && python train.py --model_id=\"customHopper_source_lr_schedule_exponential\" --env=CustomHopper-source-v0 --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmP15-DB7yVv"
      },
      "source": [
        "#### Testing the different LR schedules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_l-hhuo71fi",
        "outputId": "a3c16d44-774e-412c-8b1c-b3c9a237d9ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== TESTING CUSTOM HOPPER MODELS WITH DIFFERENT LR SCHEDULES =====\n",
            "\n",
            "----- Constant LR -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_source_lr_schedule_constant.mp4\n",
            "Test reward (avg +/- std): (1706.7078160890396 +/- 96.13198545528981) - Num episodes: 1000\n",
            "\n",
            "----- Linear LR scheduler -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_source_lr_schedule_linear.mp4\n",
            "Test reward (avg +/- std): (1811.0124509290624 +/- 61.16579476494211) - Num episodes: 1000\n",
            "\n",
            "----- Exponential LR scheduler -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_source_lr_schedule_exponential.mp4\n",
            "Test reward (avg +/- std): (1466.7613401775973 +/- 310.1469499175551) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n===== TESTING CUSTOM HOPPER MODELS WITH DIFFERENT LR SCHEDULES =====\")\n",
        "\n",
        "print(\"\\n----- Constant LR -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_source_lr_schedule_constant\" --env=CustomHopper-source-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_source_lr_schedule_constant.mp4\"\n",
        "\n",
        "print(\"\\n----- Linear LR scheduler -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_source_lr_schedule_linear\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_source_lr_schedule_linear.mp4\"\n",
        "\n",
        "print(\"\\n----- Exponential LR scheduler -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_source_lr_schedule_exponential\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_source_lr_schedule_exponential.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol7fx5YykuW4"
      },
      "source": [
        "## 2. Study the effect of single mass randomization (Hopper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zQcStPxgkzQr",
        "outputId": "88e0f911-0978-4fd4-f26b-5533d3480942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=527.13 +/- 521.87\n",
            "Episode length: 164.00 +/- 130.79\n",
            "Eval num_timesteps=1700000, episode_reward=1672.68 +/- 102.34\n",
            "Episode length: 476.40 +/- 47.20\n",
            "Eval num_timesteps=1800000, episode_reward=1470.21 +/- 292.08\n",
            "Episode length: 396.40 +/- 86.49\n",
            "Eval num_timesteps=1900000, episode_reward=1681.30 +/- 177.33\n",
            "Episode length: 467.40 +/- 65.20\n",
            "Eval num_timesteps=2000000, episode_reward=1769.59 +/- 39.65\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Training time: 3473.8241922855377\n",
            "Test reward (avg +/- std): (1599.8340286678326 +/- 258.0012141317806) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "randomization_types = [\"uniform\", \"normal\"]\n",
        "randomize_masses = [\"randomize_thigh\", \"randomize_leg\", \"randomize_foot\"]\n",
        "\n",
        "for domain_rand in randomization_types:\n",
        "    for mass in randomize_masses:\n",
        "        print(f\"Randomization type: {domain_rand} - Randomizing mass: {mass}\")\n",
        "        # name of the model\n",
        "        model_id = f\"source_randomized_{mass.split('_')[1]}_{domain_rand}_1\"\n",
        "\n",
        "        !cd sim2real && python train.py --model_id={model_id} --domain_rand={domain_rand} --{mass} --env=CustomHopper-source-v0 --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fhWrLCFjuEAv",
        "outputId": "d88becee-33cc-4466-8fe4-27ef91d3d92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Randomization type: uniform - Randomizing mass: randomize_thigh\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_uniform_thigh_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1459.0225557492859 +/- 101.61108837663537) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_uniform_thigh_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1466.2309842430482 +/- 112.99349119251956) - Num episodes: 1000\n",
            "\n",
            "Randomization type: uniform - Randomizing mass: randomize_leg\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_uniform_leg_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1721.9927571648263 +/- 104.93903780154353) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_uniform_leg_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1723.1341117016257 +/- 103.06011278500056) - Num episodes: 1000\n",
            "\n",
            "Randomization type: uniform - Randomizing mass: randomize_foot\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_uniform_foot_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1320.7107148962845 +/- 268.687334174712) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_uniform_foot_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1321.97015620687944 +/- 271.89892149809606) - Num episodes: 1000\n",
            "\n",
            "Randomization type: normal - Randomizing mass: randomize_thigh\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_normal_thigh_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1725.2114211088988 +/- 17.973113135573286) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_normal_thigh_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1725.12068047103 +/- 17.601596992454642) - Num episodes: 1000\n",
            "\n",
            "Randomization type: normal - Randomizing mass: randomize_leg\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_normal_leg_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1791.625960777059 +/- 246.64218347210803) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_normal_leg_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1786.4527157522248 +/- 261.22217855470376) - Num episodes: 1000\n",
            "\n",
            "Randomization type: normal - Randomizing mass: randomize_foot\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_normal_foot_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1768.9119040880692 +/- 53.579326480839605) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Test video save as videos/hopper_randomized_normal_foot_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1769.6066407485087 +/- 52.634088060115204) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# Test the models\n",
        "randomization_types = [\"uniform\", \"normal\"]\n",
        "randomize_masses = [\"randomize_thigh\", \"randomize_leg\", \"randomize_foot\"]\n",
        "\n",
        "for domain_rand in randomization_types:\n",
        "    for mass in randomize_masses:\n",
        "        print()\n",
        "        print(f\"Randomization type: {domain_rand} - Randomizing mass: {mass}\")\n",
        "        # name of the model\n",
        "        model_name = f\"models/PPO_model_source_randomized_{mass.split('_')[1]}_{domain_rand}_1\"\n",
        "        video_name_source_to_source = f\"hopper_randomized_{domain_rand}_{mass.split('_')[1]}_source_to_source_test_video.mp4\"\n",
        "        video_name_source_to_target = f\"hopper_randomized_{domain_rand}_{mass.split('_')[1]}_source_to_target_test_video.mp4\"\n",
        "\n",
        "        print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "        !cd sim2real && python train.py --test={model_name} --env=CustomHopper-source-v0 --test_episodes=1000 --test_rendering --video_name={video_name_source_to_source}\n",
        "\n",
        "        print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "        !cd sim2real && python train.py --test={model_name} --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name={video_name_source_to_target}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYTkK9LBNbNk"
      },
      "source": [
        "## 3. ThinHopper: Training on Custom Hopper Environment by Shifting Torso Mass by 2kg Relative to the Target Domain\n",
        "\n",
        "**Note:**\n",
        "\n",
        "Before conducting these training and test sessions, modify the following line in the `custom_hopper.py` file:\n",
        "\n",
        "`self.sim.model.body_mass[1] -= 1.0`\n",
        "\n",
        "to:\n",
        "\n",
        "`self.sim.model.body_mass[1] -= 2.0`\n",
        "\n",
        "and revert it if you wish to return to the standard configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wsTTqnyKqUIP",
        "outputId": "7cc12aaf-ffbf-43fa-d189-4c37f705d707"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- WORKING ON CUSTOM HOPPER SOURCE ENVIRONMENT ---\n",
            "\n",
            "Eval num_timesteps=100000, episode_reward=538.64 +/- 10.68\n",
            "Episode length: 172.20 +/- 1.47\n",
            "New best mean reward!\n",
            "Eval num_timesteps=200000, episode_reward=517.71 +/- 19.57\n",
            "Episode length: 162.20 +/- 3.76\n",
            "Eval num_timesteps=300000, episode_reward=675.42 +/- 7.98\n",
            "Episode length: 188.80 +/- 1.33\n",
            "New best mean reward!\n",
            "Eval num_timesteps=400000, episode_reward=1246.24 +/- 160.82\n",
            "Episode length: 390.60 +/- 50.35\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500000, episode_reward=917.72 +/- 34.56\n",
            "Episode length: 272.60 +/- 11.04\n",
            "Eval num_timesteps=600000, episode_reward=1311.55 +/- 300.74\n",
            "Episode length: 387.40 +/- 95.43\n",
            "New best mean reward!\n",
            "Eval num_timesteps=700000, episode_reward=1288.59 +/- 262.99\n",
            "Episode length: 364.60 +/- 74.66\n",
            "Eval num_timesteps=800000, episode_reward=1428.41 +/- 276.42\n",
            "Episode length: 423.80 +/- 93.33\n",
            "New best mean reward!\n",
            "Eval num_timesteps=900000, episode_reward=1549.30 +/- 162.88\n",
            "Episode length: 469.20 +/- 41.24\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000000, episode_reward=1154.04 +/- 279.25\n",
            "Episode length: 364.20 +/- 70.81\n",
            "Eval num_timesteps=1100000, episode_reward=1191.31 +/- 46.27\n",
            "Episode length: 332.80 +/- 12.25\n",
            "Eval num_timesteps=1200000, episode_reward=1665.27 +/- 164.50\n",
            "Episode length: 470.60 +/- 58.80\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1300000, episode_reward=1469.19 +/- 298.07\n",
            "Episode length: 406.60 +/- 80.57\n",
            "Eval num_timesteps=1400000, episode_reward=1667.59 +/- 12.45\n",
            "Episode length: 477.00 +/- 28.47\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1500000, episode_reward=1508.39 +/- 57.97\n",
            "Episode length: 404.20 +/- 12.94\n",
            "Eval num_timesteps=1600000, episode_reward=1641.91 +/- 154.47\n",
            "Episode length: 455.40 +/- 49.69\n",
            "Eval num_timesteps=1700000, episode_reward=1434.20 +/- 186.86\n",
            "Episode length: 371.40 +/- 44.14\n",
            "Eval num_timesteps=1800000, episode_reward=1776.33 +/- 62.14\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1900000, episode_reward=1702.69 +/- 217.70\n",
            "Episode length: 462.80 +/- 68.09\n",
            "Eval num_timesteps=2000000, episode_reward=1264.93 +/- 186.62\n",
            "Episode length: 328.00 +/- 45.58\n",
            "Training time: 3738.812102794647\n",
            "Test reward (avg +/- std): (1368.6465246274256 +/- 353.97030308759815) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on SOURCE environment without Domain Randomization\n",
        "# Source Torso Mass = Target Torso Mass - 2\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --model_id=\"customHopper_source_Torso-2\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AdmpVeNtNbNk",
        "outputId": "7fe638a7-8af5-486c-e1f3-31d56150ef67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1735.94 +/- 29.62\n",
            "Episode length: 487.40 +/- 25.20\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1700000, episode_reward=1085.43 +/- 761.76\n",
            "Episode length: 309.00 +/- 192.92\n",
            "Eval num_timesteps=1800000, episode_reward=1420.31 +/- 208.29\n",
            "Episode length: 384.20 +/- 63.89\n",
            "Eval num_timesteps=1900000, episode_reward=1222.55 +/- 353.50\n",
            "Episode length: 333.40 +/- 101.59\n",
            "Eval num_timesteps=2000000, episode_reward=1405.96 +/- 278.32\n",
            "Episode length: 374.20 +/- 81.46\n",
            "Training time: 3640.432469844818\n",
            "Test reward (avg +/- std): (1514.802774384299 +/- 303.7605455648267) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on SOURCE environment with Uniform Domain Randomization (UDR)\n",
        "# Source Torso Mass = Target Torso Mass - 2\n",
        "!cd sim2real; python train.py --env=CustomHopper-source-v0 --domain_rand=uniform --model_id=\"customHopper_source_Torso-2\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hLhyF1g8NbNk",
        "outputId": "627ea6e7-9e02-47d2-a1e7-806a2f30e868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1038.64 +/- 160.64\n",
            "Episode length: 291.60 +/- 39.75\n",
            "Eval num_timesteps=1700000, episode_reward=1576.91 +/- 269.28\n",
            "Episode length: 453.80 +/- 81.39\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1800000, episode_reward=1264.98 +/- 336.28\n",
            "Episode length: 355.00 +/- 99.35\n",
            "Eval num_timesteps=1900000, episode_reward=1228.60 +/- 286.89\n",
            "Episode length: 335.20 +/- 72.78\n",
            "Eval num_timesteps=2000000, episode_reward=1545.48 +/- 184.79\n",
            "Episode length: 442.40 +/- 70.95\n",
            "Training time: 3656.999368906021\n",
            "Test reward (avg +/- std): (1471.2034422351117 +/- 278.5423733257934) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on TARGET environment\n",
        "# Source Torso Mass = Target Torso Mass - 2\n",
        "!cd sim2real; python train.py --env=CustomHopper-target-v0 --model_id=\"customHopper_target_Torso-2\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miXfMLifNbNk"
      },
      "source": [
        "Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QIFoXHoNrLwE",
        "outputId": "69a2bebb-25d3-44c0-d83f-fe0bb1ba62e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TESTING CUSTOM HOPPER MODELS (without DR) =====\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_Torso-2_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (1190.825767679774 +/- 154.54275328296958) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_Torso-2_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (1190.5782180375388 +/- 160.12238760127784) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n===== TESTING CUSTOM HOPPER MODELS (without DR) =====\")\n",
        "\n",
        "# SOURCE ➔ SOURCE\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_source_Torso-2\" --env=CustomHopper-source-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_Torso-2_source_to_source_test.mp4\"\n",
        "\n",
        "# SOURCE ➔ TARGET\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_source_Torso-2\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_Torso-2_source_to_target_test.mp4\"\n",
        "\n",
        "# TARGET ➔ TARGET\n",
        "# print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "# !cd sim2real; python train.py --test=\"models/PPO_model_customHopper_target_Torso-2\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_Torso-2_target_to_target_test.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6NvOHohMNbNk",
        "outputId": "658f3d23-2943-4a74-89be-b86d402fd5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TESTING CUSTOM HOPPER MODELS (UDR) =====\n",
            "\n",
            "----- SOURCE (with UDR) ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_Torso-2_udr_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (1409.5016840324822 +/- 251.97632647441876) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE (with UDR) ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_Torso-2_udr_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (1416.2140568271516 +/- 256.3556054363525) - Num episodes: 1000\n",
            "\n",
            "----- TARGET ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/customHopper_Torso-2_target_to_target_test.mp4\n",
            "Test reward (avg +/- std): (1585.53176122092 +/- 236.06167791319103) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n===== TESTING CUSTOM HOPPER MODELS (UDR) =====\")\n",
        "\n",
        "# SOURCE ➔ SOURCE\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_source_Torso-2_udr\" --env=CustomHopper-source-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_Torso-2_udr_source_to_source_test.mp4\"\n",
        "\n",
        "# SOURCE ➔ TARGET\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_source_Torso-2_udr\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_Torso-2_udr_source_to_target_test.mp4\"\n",
        "\n",
        "# TARGET ➔ TARGET\n",
        "print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_customHopper_target_Torso-2_udr\" --env=CustomHopper-target-v0 --test_episodes=1000 --test_rendering --video_name=\"customHopper_Torso-2_target_to_target_test.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgN6hWE-kptP"
      },
      "source": [
        "## 4. Add new environment: Walker2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i20b8lUakiQb"
      },
      "source": [
        "### Training on Custom Walker2D environment using PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GjNrq7GnkOZJ",
        "outputId": "bbac89de-52b4-48da-a1e2-30c431aad416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=2929.04 +/- 736.45\n",
            "Episode length: 785.20 +/- 189.82\n",
            "Eval num_timesteps=1700000, episode_reward=2691.23 +/- 1148.30\n",
            "Episode length: 720.80 +/- 290.36\n",
            "Eval num_timesteps=1800000, episode_reward=2097.49 +/- 1507.20\n",
            "Episode length: 554.60 +/- 364.74\n",
            "Eval num_timesteps=1900000, episode_reward=1781.86 +/- 258.93\n",
            "Episode length: 495.20 +/- 76.48\n",
            "Eval num_timesteps=2000000, episode_reward=3361.88 +/- 852.17\n",
            "Episode length: 874.40 +/- 211.71\n",
            "New best mean reward!\n",
            "Training time: 4247.864328622818\n",
            "Test reward (avg +/- std): (2052.202233350383 +/- 1001.7444958567927) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on source environment (2M timesteps)\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --model_id=\"walker2d_source_1\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LKCKXLv92Tzc",
        "outputId": "c71c64ea-d8a1-4e6a-b723-b6470e27c6f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1151.61 +/- 250.39\n",
            "Episode length: 330.60 +/- 67.38\n",
            "Eval num_timesteps=1700000, episode_reward=1105.69 +/- 632.67\n",
            "Episode length: 334.80 +/- 196.25\n",
            "Eval num_timesteps=1800000, episode_reward=1134.83 +/- 328.08\n",
            "Episode length: 334.80 +/- 71.86\n",
            "Eval num_timesteps=1900000, episode_reward=804.65 +/- 338.75\n",
            "Episode length: 240.40 +/- 81.51\n",
            "Eval num_timesteps=2000000, episode_reward=1019.71 +/- 262.54\n",
            "Episode length: 304.80 +/- 60.24\n",
            "Training time: 4263.223496198654\n",
            "Test reward (avg +/- std): (1235.8550028249076 +/- 439.848499813264) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on target environment (2M timesteps)\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-target-v0 --model_id=\"walker2d_target_1\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgSt9W45sO0C"
      },
      "source": [
        "### Training on Custom Walker2D environment, with complete domain randomization (all the masses except the torso), using PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IwxJ2ZlasaNO",
        "outputId": "1c7002f9-ba09-4454-c01a-7cc83c9af193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1547.27 +/- 1099.27\n",
            "Episode length: 452.20 +/- 280.28\n",
            "Eval num_timesteps=1700000, episode_reward=2546.70 +/- 1262.40\n",
            "Episode length: 716.40 +/- 349.14\n",
            "Eval num_timesteps=1800000, episode_reward=2462.83 +/- 1349.13\n",
            "Episode length: 702.60 +/- 365.93\n",
            "Eval num_timesteps=1900000, episode_reward=3586.17 +/- 107.74\n",
            "Episode length: 993.00 +/- 14.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000000, episode_reward=2282.12 +/- 1081.24\n",
            "Episode length: 624.40 +/- 278.25\n",
            "Training time: 3766.763552427292\n",
            "Test reward (avg +/- std): (2548.07213067711 +/- 1217.4967016170572) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training using Uniform Domain Randomization (all masses except the torso)\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --model_id=\"walker2d_source_udr_uniform_1\" --domain_rand=uniform --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1t_rTWc_sfXQ",
        "outputId": "e4f7eac3-2b74-47b2-f0e1-f16aaf60c50a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1136.91 +/- 271.86\n",
            "Episode length: 357.60 +/- 60.79\n",
            "Eval num_timesteps=1700000, episode_reward=1766.80 +/- 648.77\n",
            "Episode length: 525.80 +/- 208.62\n",
            "Eval num_timesteps=1800000, episode_reward=1837.84 +/- 890.00\n",
            "Episode length: 578.00 +/- 265.57\n",
            "Eval num_timesteps=1900000, episode_reward=1526.10 +/- 880.21\n",
            "Episode length: 482.40 +/- 267.11\n",
            "Eval num_timesteps=2000000, episode_reward=1118.11 +/- 331.35\n",
            "Episode length: 323.20 +/- 81.42\n",
            "Training time: 3788.8086972236633\n",
            "Test reward (avg +/- std): (1433.6702532590398 +/- 696.907142093376) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "#Training using Truncated Normal Domain Randomization (all masses except the torso)\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --model_id=\"walker2d_source_udr_normal_1\" --domain_rand=normal --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nENBo54WUdxS"
      },
      "source": [
        "### Training on Custom Walker2D environment, with single mass randomization, using PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KUwosOAkUo8T",
        "outputId": "f54bacd6-2584-4bc6-ef3b-8595ca4ca6e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Walker2D - Environment: CustomWalker2D-source-v0 - Randomization: uniform - Mass: randomize_thigh\n",
            "\n",
            "--- WORKING ON CUSTOM WALKER 2D SOURCE ENVIRONMENT ---\n",
            "\n",
            "Eval num_timesteps=100000, episode_reward=283.34 +/- 6.53\n",
            "Episode length: 161.20 +/- 6.05\n",
            "New best mean reward!\n",
            "Eval num_timesteps=200000, episode_reward=372.42 +/- 5.57\n",
            "Episode length: 184.20 +/- 4.45\n",
            "New best mean reward!\n",
            "Eval num_timesteps=300000, episode_reward=453.64 +/- 4.11\n",
            "Episode length: 194.20 +/- 1.72\n",
            "New best mean reward!\n",
            "Eval num_timesteps=400000, episode_reward=550.83 +/- 36.47\n",
            "Episode length: 205.40 +/- 19.09\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500000, episode_reward=1876.98 +/- 797.39\n",
            "Episode length: 726.40 +/- 335.38\n",
            "New best mean reward!\n",
            "Eval num_timesteps=600000, episode_reward=1668.96 +/- 874.06\n",
            "Episode length: 696.60 +/- 372.27\n",
            "Eval num_timesteps=700000, episode_reward=905.95 +/- 70.41\n",
            "Episode length: 311.40 +/- 23.25\n",
            "Eval num_timesteps=800000, episode_reward=1949.02 +/- 701.43\n",
            "Episode length: 653.20 +/- 247.61\n",
            "New best mean reward!\n",
            "Eval num_timesteps=900000, episode_reward=2363.31 +/- 589.50\n",
            "Episode length: 795.20 +/- 196.69\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000000, episode_reward=1378.38 +/- 424.99\n",
            "Episode length: 469.60 +/- 122.05\n",
            "Eval num_timesteps=1100000, episode_reward=1853.56 +/- 584.98\n",
            "Episode length: 600.00 +/- 214.32\n",
            "Eval num_timesteps=1200000, episode_reward=1776.72 +/- 1146.28\n",
            "Episode length: 556.40 +/- 365.31\n",
            "Eval num_timesteps=1300000, episode_reward=2077.92 +/- 567.96\n",
            "Episode length: 605.60 +/- 150.24\n",
            "Eval num_timesteps=1400000, episode_reward=2534.39 +/- 973.55\n",
            "Episode length: 732.80 +/- 268.12\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1500000, episode_reward=2781.58 +/- 623.51\n",
            "Episode length: 780.40 +/- 183.66\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1600000, episode_reward=3297.63 +/- 71.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1700000, episode_reward=2910.00 +/- 809.09\n",
            "Episode length: 817.40 +/- 224.17\n",
            "Eval num_timesteps=1800000, episode_reward=2565.15 +/- 840.04\n",
            "Episode length: 704.80 +/- 241.06\n",
            "Eval num_timesteps=1900000, episode_reward=1849.31 +/- 900.00\n",
            "Episode length: 517.60 +/- 227.33\n",
            "Eval num_timesteps=2000000, episode_reward=2627.38 +/- 1217.46\n",
            "Episode length: 726.00 +/- 335.67\n",
            "Training time: 4392.212657213211\n",
            "Test reward (avg +/- std): (1814.48570170316 +/- 1251.2854153628475) - Num episodes: 100\n",
            "Training Walker2D - Environment: CustomWalker2D-source-v0 - Randomization: uniform - Mass: randomize_leg\n",
            "\n",
            "--- WORKING ON CUSTOM WALKER 2D SOURCE ENVIRONMENT ---\n",
            "\n",
            "Eval num_timesteps=100000, episode_reward=266.42 +/- 3.79\n",
            "Episode length: 145.40 +/- 3.01\n",
            "New best mean reward!\n",
            "Eval num_timesteps=200000, episode_reward=258.85 +/- 10.81\n",
            "Episode length: 134.80 +/- 7.68\n",
            "Eval num_timesteps=300000, episode_reward=431.83 +/- 11.67\n",
            "Episode length: 200.00 +/- 6.93\n",
            "New best mean reward!\n",
            "Eval num_timesteps=400000, episode_reward=548.21 +/- 6.37\n",
            "Episode length: 220.80 +/- 6.43\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500000, episode_reward=664.09 +/- 21.21\n",
            "Episode length: 249.60 +/- 8.01\n",
            "New best mean reward!\n",
            "Eval num_timesteps=600000, episode_reward=712.65 +/- 11.33\n",
            "Episode length: 246.20 +/- 6.01\n",
            "New best mean reward!\n",
            "Eval num_timesteps=700000, episode_reward=1453.31 +/- 445.93\n",
            "Episode length: 662.80 +/- 242.08\n",
            "New best mean reward!\n",
            "Eval num_timesteps=800000, episode_reward=1386.62 +/- 651.20\n",
            "Episode length: 469.60 +/- 263.48\n",
            "Eval num_timesteps=900000, episode_reward=1698.05 +/- 1190.12\n",
            "Episode length: 475.60 +/- 281.35\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000000, episode_reward=1236.82 +/- 576.94\n",
            "Episode length: 327.20 +/- 123.95\n",
            "Eval num_timesteps=1100000, episode_reward=2752.47 +/- 1046.09\n",
            "Episode length: 683.00 +/- 265.46\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1200000, episode_reward=1649.92 +/- 903.32\n",
            "Episode length: 464.20 +/- 267.99\n",
            "Eval num_timesteps=1300000, episode_reward=4015.45 +/- 415.64\n",
            "Episode length: 960.40 +/- 79.20\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1400000, episode_reward=2344.12 +/- 997.98\n",
            "Episode length: 575.40 +/- 222.99\n",
            "Eval num_timesteps=1500000, episode_reward=4562.10 +/- 176.71\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1600000, episode_reward=2964.48 +/- 2046.53\n",
            "Episode length: 663.80 +/- 411.76\n",
            "Eval num_timesteps=1700000, episode_reward=3091.58 +/- 1664.03\n",
            "Episode length: 655.80 +/- 310.42\n",
            "Eval num_timesteps=1800000, episode_reward=3907.53 +/- 1725.74\n",
            "Episode length: 832.20 +/- 335.60\n",
            "Eval num_timesteps=1900000, episode_reward=4919.23 +/- 114.23\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000000, episode_reward=4505.31 +/- 497.06\n",
            "Episode length: 927.40 +/- 91.12\n",
            "Training time: 4364.6977434158325\n",
            "Test reward (avg +/- std): (3123.260112312801 +/- 1735.4284546067668) - Num episodes: 100\n",
            "Training Walker2D - Environment: CustomWalker2D-source-v0 - Randomization: uniform - Mass: randomize_foot\n",
            "\n",
            "--- WORKING ON CUSTOM WALKER 2D SOURCE ENVIRONMENT ---\n",
            "\n",
            "Eval num_timesteps=100000, episode_reward=284.21 +/- 12.37\n",
            "Episode length: 149.20 +/- 10.11\n",
            "New best mean reward!\n",
            "Eval num_timesteps=200000, episode_reward=408.34 +/- 7.22\n",
            "Episode length: 214.00 +/- 6.60\n",
            "New best mean reward!\n",
            "Eval num_timesteps=300000, episode_reward=563.64 +/- 8.73\n",
            "Episode length: 260.00 +/- 4.34\n",
            "New best mean reward!\n",
            "Eval num_timesteps=400000, episode_reward=680.66 +/- 32.52\n",
            "Episode length: 259.20 +/- 15.64\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500000, episode_reward=1608.28 +/- 719.64\n",
            "Episode length: 567.60 +/- 257.80\n",
            "New best mean reward!\n",
            "Eval num_timesteps=600000, episode_reward=2662.06 +/- 21.24\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=700000, episode_reward=3036.82 +/- 24.76\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=800000, episode_reward=3271.99 +/- 445.94\n",
            "Episode length: 931.20 +/- 137.60\n",
            "New best mean reward!\n",
            "Eval num_timesteps=900000, episode_reward=1909.83 +/- 1173.67\n",
            "Episode length: 573.00 +/- 350.52\n",
            "Eval num_timesteps=1000000, episode_reward=2588.39 +/- 1222.94\n",
            "Episode length: 738.40 +/- 331.25\n",
            "Eval num_timesteps=1100000, episode_reward=3262.29 +/- 858.46\n",
            "Episode length: 883.40 +/- 233.20\n",
            "Eval num_timesteps=1200000, episode_reward=373.71 +/- 190.84\n",
            "Episode length: 149.20 +/- 48.11\n",
            "Eval num_timesteps=1300000, episode_reward=386.28 +/- 155.72\n",
            "Episode length: 150.20 +/- 35.87\n",
            "Eval num_timesteps=1400000, episode_reward=1154.64 +/- 1467.08\n",
            "Episode length: 326.20 +/- 339.08\n",
            "Eval num_timesteps=1500000, episode_reward=1842.15 +/- 1247.71\n",
            "Episode length: 469.80 +/- 266.13\n",
            "Eval num_timesteps=1600000, episode_reward=3496.70 +/- 905.04\n",
            "Episode length: 840.20 +/- 197.55\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1700000, episode_reward=1927.88 +/- 1221.61\n",
            "Episode length: 495.00 +/- 275.38\n",
            "Eval num_timesteps=1800000, episode_reward=1922.48 +/- 2000.83\n",
            "Episode length: 472.00 +/- 431.24\n",
            "Eval num_timesteps=1900000, episode_reward=478.70 +/- 153.86\n",
            "Episode length: 158.80 +/- 34.94\n",
            "Eval num_timesteps=2000000, episode_reward=3659.35 +/- 1580.27\n",
            "Episode length: 832.60 +/- 334.80\n",
            "New best mean reward!\n",
            "Training time: 4342.407379865646\n",
            "Test reward (avg +/- std): (2635.8972392670903 +/- 1144.0694630593343) - Num episodes: 100\n",
            "Training Walker2D - Environment: CustomWalker2D-source-v0 - Randomization: normal - Mass: randomize_thigh\n",
            "\n",
            "--- WORKING ON CUSTOM WALKER 2D SOURCE ENVIRONMENT ---\n",
            "\n",
            "Eval num_timesteps=100000, episode_reward=255.56 +/- 4.84\n",
            "Episode length: 137.20 +/- 3.97\n",
            "New best mean reward!\n",
            "Eval num_timesteps=200000, episode_reward=360.00 +/- 8.24\n",
            "Episode length: 195.60 +/- 5.35\n",
            "New best mean reward!\n",
            "Eval num_timesteps=300000, episode_reward=414.40 +/- 4.13\n",
            "Episode length: 200.80 +/- 3.87\n",
            "New best mean reward!\n",
            "Eval num_timesteps=400000, episode_reward=444.61 +/- 7.53\n",
            "Episode length: 199.20 +/- 3.37\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500000, episode_reward=557.40 +/- 19.78\n",
            "Episode length: 211.40 +/- 12.89\n",
            "New best mean reward!\n",
            "Eval num_timesteps=600000, episode_reward=958.44 +/- 54.12\n",
            "Episode length: 353.20 +/- 8.11\n",
            "New best mean reward!\n",
            "Eval num_timesteps=700000, episode_reward=1155.96 +/- 765.58\n",
            "Episode length: 406.40 +/- 297.57\n",
            "New best mean reward!\n",
            "Eval num_timesteps=800000, episode_reward=1489.01 +/- 708.72\n",
            "Episode length: 476.20 +/- 219.98\n",
            "New best mean reward!\n",
            "Eval num_timesteps=900000, episode_reward=2944.92 +/- 1013.20\n",
            "Episode length: 854.60 +/- 290.80\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000000, episode_reward=2592.58 +/- 1109.40\n",
            "Episode length: 737.40 +/- 322.02\n",
            "Eval num_timesteps=1100000, episode_reward=3244.72 +/- 671.23\n",
            "Episode length: 905.40 +/- 189.20\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1200000, episode_reward=3630.99 +/- 20.81\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1300000, episode_reward=2410.29 +/- 1123.64\n",
            "Episode length: 653.60 +/- 298.30\n",
            "Eval num_timesteps=1400000, episode_reward=2803.64 +/- 1356.44\n",
            "Episode length: 726.80 +/- 334.61\n",
            "Eval num_timesteps=1500000, episode_reward=3186.85 +/- 1101.94\n",
            "Episode length: 855.00 +/- 290.00\n",
            "Eval num_timesteps=1600000, episode_reward=3857.56 +/- 71.69\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1700000, episode_reward=1332.31 +/- 1328.61\n",
            "Episode length: 368.00 +/- 316.08\n",
            "Eval num_timesteps=1800000, episode_reward=3390.34 +/- 1320.75\n",
            "Episode length: 845.40 +/- 309.20\n",
            "Eval num_timesteps=1900000, episode_reward=3660.76 +/- 930.08\n",
            "Episode length: 884.60 +/- 217.55\n",
            "Eval num_timesteps=2000000, episode_reward=2683.81 +/- 1619.45\n",
            "Episode length: 688.00 +/- 382.21\n",
            "Training time: 4478.198545455933\n",
            "Test reward (avg +/- std): (2104.000621858408 +/- 1401.256873358409) - Num episodes: 100\n",
            "Training Walker2D - Environment: CustomWalker2D-source-v0 - Randomization: normal - Mass: randomize_leg\n",
            "\n",
            "--- WORKING ON CUSTOM WALKER 2D SOURCE ENVIRONMENT ---\n",
            "\n",
            "Eval num_timesteps=100000, episode_reward=284.50 +/- 2.45\n",
            "Episode length: 151.80 +/- 2.48\n",
            "New best mean reward!\n",
            "Eval num_timesteps=200000, episode_reward=406.01 +/- 7.89\n",
            "Episode length: 202.40 +/- 3.93\n",
            "New best mean reward!\n",
            "Eval num_timesteps=300000, episode_reward=543.79 +/- 5.88\n",
            "Episode length: 236.60 +/- 4.08\n",
            "New best mean reward!\n",
            "Eval num_timesteps=400000, episode_reward=757.82 +/- 41.17\n",
            "Episode length: 310.20 +/- 21.89\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500000, episode_reward=792.48 +/- 62.05\n",
            "Episode length: 306.40 +/- 13.69\n",
            "New best mean reward!\n",
            "Eval num_timesteps=600000, episode_reward=899.96 +/- 99.28\n",
            "Episode length: 315.20 +/- 29.61\n",
            "New best mean reward!\n",
            "Eval num_timesteps=700000, episode_reward=870.03 +/- 65.16\n",
            "Episode length: 310.60 +/- 21.46\n",
            "Eval num_timesteps=800000, episode_reward=939.93 +/- 101.11\n",
            "Episode length: 294.20 +/- 30.85\n",
            "New best mean reward!\n",
            "Eval num_timesteps=900000, episode_reward=831.50 +/- 44.23\n",
            "Episode length: 261.20 +/- 11.81\n",
            "Eval num_timesteps=1000000, episode_reward=734.18 +/- 22.27\n",
            "Episode length: 227.20 +/- 9.39\n",
            "Eval num_timesteps=1100000, episode_reward=897.14 +/- 140.46\n",
            "Episode length: 277.00 +/- 43.04\n",
            "Eval num_timesteps=1200000, episode_reward=986.65 +/- 191.21\n",
            "Episode length: 294.60 +/- 54.92\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1300000, episode_reward=952.31 +/- 101.89\n",
            "Episode length: 283.20 +/- 31.83\n",
            "Eval num_timesteps=1400000, episode_reward=1071.02 +/- 172.67\n",
            "Episode length: 305.40 +/- 44.55\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1500000, episode_reward=1351.41 +/- 121.85\n",
            "Episode length: 391.60 +/- 40.48\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1600000, episode_reward=1069.33 +/- 376.28\n",
            "Episode length: 303.20 +/- 84.17\n",
            "Eval num_timesteps=1700000, episode_reward=1219.40 +/- 996.43\n",
            "Episode length: 380.00 +/- 312.76\n",
            "Eval num_timesteps=1800000, episode_reward=1235.19 +/- 448.46\n",
            "Episode length: 371.60 +/- 150.00\n",
            "Eval num_timesteps=1900000, episode_reward=952.86 +/- 199.72\n",
            "Episode length: 282.00 +/- 46.21\n",
            "Eval num_timesteps=2000000, episode_reward=1227.07 +/- 712.19\n",
            "Episode length: 372.20 +/- 216.69\n",
            "Training time: 4506.706141233444\n",
            "Test reward (avg +/- std): (1480.6354574357892 +/- 672.7574187052934) - Num episodes: 100\n",
            "Training Walker2D - Environment: CustomWalker2D-source-v0 - Randomization: normal - Mass: randomize_foot\n",
            "\n",
            "--- WORKING ON CUSTOM WALKER 2D SOURCE ENVIRONMENT ---\n",
            "\n",
            "Eval num_timesteps=100000, episode_reward=293.92 +/- 16.69\n",
            "Episode length: 168.20 +/- 16.29\n",
            "New best mean reward!\n",
            "Eval num_timesteps=200000, episode_reward=171.84 +/- 105.01\n",
            "Episode length: 186.80 +/- 28.25\n",
            "Eval num_timesteps=300000, episode_reward=384.66 +/- 24.03\n",
            "Episode length: 197.80 +/- 19.25\n",
            "New best mean reward!\n",
            "Eval num_timesteps=400000, episode_reward=542.69 +/- 21.31\n",
            "Episode length: 230.80 +/- 11.18\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500000, episode_reward=602.95 +/- 53.74\n",
            "Episode length: 244.80 +/- 24.27\n",
            "New best mean reward!\n",
            "Eval num_timesteps=600000, episode_reward=748.08 +/- 202.95\n",
            "Episode length: 308.80 +/- 98.13\n",
            "New best mean reward!\n",
            "Eval num_timesteps=700000, episode_reward=678.46 +/- 159.95\n",
            "Episode length: 283.60 +/- 88.69\n",
            "Eval num_timesteps=800000, episode_reward=1730.22 +/- 402.62\n",
            "Episode length: 729.80 +/- 170.22\n",
            "New best mean reward!\n",
            "Eval num_timesteps=900000, episode_reward=887.58 +/- 311.16\n",
            "Episode length: 346.20 +/- 114.85\n",
            "Eval num_timesteps=1000000, episode_reward=829.86 +/- 233.08\n",
            "Episode length: 337.00 +/- 63.17\n",
            "Eval num_timesteps=1100000, episode_reward=1387.23 +/- 619.98\n",
            "Episode length: 582.60 +/- 263.02\n",
            "Eval num_timesteps=1200000, episode_reward=849.93 +/- 188.37\n",
            "Episode length: 290.80 +/- 59.29\n",
            "Eval num_timesteps=1300000, episode_reward=1710.48 +/- 605.23\n",
            "Episode length: 687.40 +/- 208.43\n",
            "Eval num_timesteps=1400000, episode_reward=1185.65 +/- 402.53\n",
            "Episode length: 385.20 +/- 140.12\n",
            "Eval num_timesteps=1500000, episode_reward=1137.49 +/- 879.52\n",
            "Episode length: 402.20 +/- 307.85\n",
            "Eval num_timesteps=1600000, episode_reward=2039.55 +/- 769.10\n",
            "Episode length: 744.60 +/- 264.39\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1700000, episode_reward=1850.97 +/- 950.24\n",
            "Episode length: 612.80 +/- 322.26\n",
            "Eval num_timesteps=1800000, episode_reward=1300.86 +/- 742.51\n",
            "Episode length: 410.80 +/- 230.41\n",
            "Eval num_timesteps=1900000, episode_reward=2217.51 +/- 975.40\n",
            "Episode length: 729.80 +/- 327.77\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000000, episode_reward=2552.09 +/- 670.23\n",
            "Episode length: 794.60 +/- 207.77\n",
            "New best mean reward!\n",
            "Training time: 4500.585993528366\n",
            "Test reward (avg +/- std): (1988.2956208844785 +/- 938.4903473449493) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "randomization_types = [\"uniform\", \"normal\"]\n",
        "randomize_masses = [\"randomize_thigh\", \"randomize_leg\", \"randomize_foot\"]\n",
        "\n",
        "walker_env = \"CustomWalker2D-source-v0\"\n",
        "\n",
        "for domain_rand in randomization_types:\n",
        "    for mass in randomize_masses:\n",
        "        model_id = f\"walker2d_source_{mass.split('_')[1]}_{domain_rand}\"\n",
        "\n",
        "        print(f\"Training Walker2D - Environment: {walker_env} - Randomization: {domain_rand} - Mass: {mass}\")\n",
        "        !cd sim2real && python train.py --env={walker_env} --model_id={model_id} --domain_rand={domain_rand} --{mass} --total_timesteps=2000000 --lr_schedule=\"exponential\" --verbose=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz6vtSvJz041"
      },
      "source": [
        "### Test models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUeiewfeXHiH"
      },
      "source": [
        "**Without mass randomization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jlmxiEAHz3dH",
        "outputId": "9cdf39a8-3a12-457f-a7d7-d32f1a7680ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (2376.825577337019 +/- 765.3996059101355) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (2039.6931135233854 +/- 940.5783974139155) - Num episodes: 1000\n",
            "\n",
            "----- TARGET ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_target_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (2293.7706885371367 +/- 453.3943173275468) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_1\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_source_to_source_test_video.mp4\"\n",
        "\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_1\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_source_to_target_test_video.mp4\"\n",
        "\n",
        "print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_target_1\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_target_to_target_test_video.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8yjfhAVRGyd"
      },
      "source": [
        "**With complete mass randomization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uKnFinhDRK-h",
        "outputId": "20f1d124-020b-4091-9897-549fa693e508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TESTING UNIFORM RANDOMIZATION =====\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_uniform_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (2854.67438447908 +/- 583.6749631509572) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_uniform_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (2861.9709338686243 +/- 576.7817310062762) - Num episodes: 1000\n",
            "\n",
            "===== TESTING NORMAL RANDOMIZATION =====\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_normal_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1605.181620032894 +/- 761.5364568541646) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_normal_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1589.144718843952 +/- 749.0685309575147) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "    \"uniform\": \"models/PPO_model_walker2d_source_udr_uniform_1.zip\",\n",
        "    \"normal\": \"models/PPO_model_walker2d_source_udr_normal_1.zip\"\n",
        "}\n",
        "\n",
        "# Test for each type of randomization\n",
        "for rand_type, model_path in models.items():\n",
        "    print(f\"\\n===== TESTING {rand_type.upper()} RANDOMIZATION =====\")\n",
        "\n",
        "    # Test SOURCE ➔ SOURCE\n",
        "    print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "    !cd sim2real; python train.py --test={model_path} --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_{rand_type}_source_to_source_test_video.mp4\"\n",
        "\n",
        "    # Test SOURCE ➔ TARGET\n",
        "    print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "    !cd sim2real; python train.py --test={model_path} --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_{rand_type}_source_to_target_test_video.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5l08JNVXT5H"
      },
      "source": [
        "**With single mass randomization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "km4pf4ZmXWXv",
        "outputId": "5b60ca65-16d9-4131-9cc6-a6f2af2ffa4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Randomization type: uniform - Randomizing mass: randomize_thigh\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_uniform_thigh_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (3360.9074214536113 +/- 785.0970548114917) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_uniform_thigh_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (3345.010270724061 +/- 812.0615812999199) - Num episodes: 1000\n",
            "\n",
            "Randomization type: uniform - Randomizing mass: randomize_leg\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_uniform_leg_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (4228.557307987444 +/- 845.0889348791384) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_uniform_leg_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (4138.719144509976 +/- 826.529116994646) - Num episodes: 1000\n",
            "\n",
            "Randomization type: uniform - Randomizing mass: randomize_foot\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_uniform_foot_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (3919.8192348906973 +/- 975.1482297943865) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_uniform_foot_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (3939.7415164070017 +/- 956.3646333238803) - Num episodes: 1000\n",
            "\n",
            "Randomization type: normal - Randomizing mass: randomize_thigh\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_normal_thigh_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (3109.3875175590724 +/- 1116.030747784883) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_normal_thigh_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (3201.0837098580523 +/- 1176.753393924145) - Num episodes: 1000\n",
            "\n",
            "Randomization type: normal - Randomizing mass: randomize_leg\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_normal_leg_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (1221.811169184799 +/- 725.5034504273984) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_normal_leg_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (1260.4998409987622 +/- 740.9694321946196) - Num episodes: 1000\n",
            "\n",
            "Randomization type: normal - Randomizing mass: randomize_foot\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_normal_foot_source_to_source_test_video.mp4\n",
            "Test reward (avg +/- std): (2656.4896765150856 +/- 779.3008627496495) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_randomized_normal_foot_source_to_target_test_video.mp4\n",
            "Test reward (avg +/- std): (2689.839560449397 +/- 772.2451041077402) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "randomization_types = [\"uniform\", \"normal\"]\n",
        "randomize_masses = [\"randomize_thigh\", \"randomize_leg\", \"randomize_foot\"]\n",
        "\n",
        "for domain_rand in randomization_types:\n",
        "    for mass in randomize_masses:\n",
        "        print()\n",
        "        print(f\"Randomization type: {domain_rand} - Randomizing mass: {mass}\")\n",
        "\n",
        "        model_name = f\"models/PPO_model_walker2d_source_{mass.split('_')[1]}_{domain_rand}\"\n",
        "        video_name_source_to_source = f\"walker2d_randomized_{domain_rand}_{mass.split('_')[1]}_source_to_source_test_video.mp4\"\n",
        "        video_name_source_to_target = f\"walker2d_randomized_{domain_rand}_{mass.split('_')[1]}_source_to_target_test_video.mp4\"\n",
        "\n",
        "        print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "        !cd sim2real && python train.py --test={model_name} --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name={video_name_source_to_source}\n",
        "\n",
        "        print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "        !cd sim2real && python train.py --test={model_name} --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name={video_name_source_to_target}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk57cA7Cu-ud"
      },
      "source": [
        "## 5. BigFoot: Training on a custom Walker2D environment with foot masses tripled compared to the standard configuration (in the source env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "afFDOyPKetPQ",
        "outputId": "81414240-52ec-4a19-af29-2db2875fdea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1825.08 +/- 197.38\n",
            "Episode length: 514.80 +/- 45.18\n",
            "Eval num_timesteps=1700000, episode_reward=2652.27 +/- 1050.10\n",
            "Episode length: 730.00 +/- 279.65\n",
            "Eval num_timesteps=1800000, episode_reward=2502.61 +/- 1206.94\n",
            "Episode length: 690.40 +/- 304.09\n",
            "Eval num_timesteps=1900000, episode_reward=2580.78 +/- 1063.27\n",
            "Episode length: 700.80 +/- 279.16\n",
            "Eval num_timesteps=2000000, episode_reward=3491.44 +/- 532.00\n",
            "Episode length: 888.60 +/- 120.64\n",
            "New best mean reward!\n",
            "Training time: 4293.380982160568\n",
            "Test reward (avg +/- std): (2165.3284343592622 +/- 887.5447310093815) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on source environment (No DR)\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --model_id=\"walker2d_source_heavy_feet_no_dr\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g4fxFQfGetPR",
        "outputId": "6448d162-4168-4ea8-a3fb-7832c13e1fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=2421.09 +/- 1118.74\n",
            "Episode length: 665.20 +/- 275.14\n",
            "Eval num_timesteps=1700000, episode_reward=3792.43 +/- 109.03\n",
            "Episode length: 983.00 +/- 34.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1800000, episode_reward=3073.00 +/- 490.45\n",
            "Episode length: 888.40 +/- 136.68\n",
            "Eval num_timesteps=1900000, episode_reward=3151.44 +/- 742.09\n",
            "Episode length: 840.80 +/- 198.19\n",
            "Eval num_timesteps=2000000, episode_reward=3101.77 +/- 1013.94\n",
            "Episode length: 840.20 +/- 265.00\n",
            "Training time: 4349.737284421921\n",
            "Test reward (avg +/- std): (1885.2291754981588 +/- 946.7469577493339) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on target environment (No DR)\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-target-v0 --model_id=\"walker2d_target_heavy_feet_no_dr\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GgOpFMEUetPR",
        "outputId": "007240d4-4569-48b3-caab-94872dcdb55c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=3327.23 +/- 745.29\n",
            "Episode length: 853.60 +/- 167.70\n",
            "Eval num_timesteps=1700000, episode_reward=3330.25 +/- 1490.77\n",
            "Episode length: 758.40 +/- 305.95\n",
            "Eval num_timesteps=1800000, episode_reward=3428.10 +/- 1626.68\n",
            "Episode length: 741.40 +/- 316.91\n",
            "Eval num_timesteps=1900000, episode_reward=3515.70 +/- 1266.67\n",
            "Episode length: 793.00 +/- 254.25\n",
            "Eval num_timesteps=2000000, episode_reward=3053.05 +/- 1522.19\n",
            "Episode length: 732.80 +/- 327.29\n",
            "Training time: 4388.302843570709\n",
            "Test reward (avg +/- std): (2610.2305453573854 +/- 1339.756606106864) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on source environment with Uniform Domain randomization\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --domain_rand=uniform --model_id=\"walker2d_source_heavy_feet_with_dr\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bhOv8wlFhLPB",
        "outputId": "9b56d4e8-779f-459b-960f-ee002ba3e58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=1997.32 +/- 1226.82\n",
            "Episode length: 487.40 +/- 257.07\n",
            "Eval num_timesteps=1700000, episode_reward=3281.67 +/- 1467.58\n",
            "Episode length: 830.60 +/- 330.37\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1800000, episode_reward=3828.55 +/- 592.40\n",
            "Episode length: 899.20 +/- 153.79\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1900000, episode_reward=3204.10 +/- 1241.59\n",
            "Episode length: 713.00 +/- 255.90\n",
            "Eval num_timesteps=2000000, episode_reward=1464.28 +/- 837.48\n",
            "Episode length: 374.60 +/- 176.18\n",
            "Training time: 4394.285305023193\n",
            "Test reward (avg +/- std): (2472.0408017628083 +/- 1532.396057900067) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on source environment with Truncated Normal Domain randomization\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --domain_rand=normal --model_id=\"walker2d_source_heavy_feet_with_dr_normal\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6ECC9gTetPR"
      },
      "source": [
        "### BigFoot: Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GSGRyVwetPR"
      },
      "source": [
        "NO Domain randomization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HltYcN9NetPR",
        "outputId": "1d6d8333-ea3d-43f9-aa5f-038b8918a9f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_heavy_feet_no_dr_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (2957.9146232488424 +/- 894.1275858110675) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_heavy_feet_no_dr_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (2894.8308793413203 +/- 896.5872505254711) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_heavy_feet_no_dr\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_heavy_feet_no_dr_source_to_source_test.mp4\"\n",
        "\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_heavy_feet_no_dr\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_heavy_feet_no_dr_source_to_target_test.mp4\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLUd4udEetPR"
      },
      "source": [
        "With domain randomization (UNIFORM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eeNt7VgNetPR",
        "outputId": "92b11a53-97fd-4bc1-dfc9-0c552c7c33c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_heavy_feet_with_dr_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (4158.027530824367 +/- 757.5385500423763) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# SOURCE --> SOURCE\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_heavy_feet_with_dr\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_heavy_feet_with_dr_source_to_source_test.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b8s05445etPR",
        "outputId": "80a77351-17b9-497b-8cee-6b7e8d700bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_heavy_feet_with_dr_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (4140.04923921812 +/- 784.2544573615471) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# SOURCE --> TARGET\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_heavy_feet_with_dr\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_heavy_feet_with_dr_source_to_target_test.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bntg0u9hZ--"
      },
      "source": [
        "With domain randomization (NORMAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qKugS3EghY-9",
        "outputId": "f84672f5-0db8-4ae1-cf01-b002a22a927d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_heavy_feet_with_dr_normal_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (2353.8253454171468 +/- 1781.6232670314496) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# SOURCE --> SOURCE\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_heavy_feet_with_dr_normal\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_heavy_feet_with_dr_normal_source_to_source_test.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yHhxrSE0hg70",
        "outputId": "01d4a432-e6f0-4171-93af-ad08b9d812b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_heavy_feet_with_dr_normal_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (2314.2939306035905 +/- 1770.9294288478968) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# SOURCE --> TARGET\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_heavy_feet_with_dr_normal\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_heavy_feet_with_dr_normal_source_to_target_test.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq1_ZjXzIaBt"
      },
      "source": [
        "## 6. Fixed seed during training of the walker2d environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT2LA5HIIaBt"
      },
      "source": [
        "**Seed Configuration**\n",
        "\n",
        "To ensure reproducibility and analyze the impact of randomness, the seed was consistently set across all components of the project:\n",
        "\n",
        "*NumPy:* For generating random numbers in numerical operations.\n",
        "\n",
        "*PyTorch:* For initializing neural network weights and other stochastic operations.\n",
        "\n",
        "*Gym:* To control randomness in the environment and action space.\n",
        "\n",
        "*Python Random Module:* For any standard random functions.\n",
        "\n",
        "The seed can be specified using the parameter `--seed`, ensuring that all randomizations share the same base, guaranteeing reproducible results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Kma5P0dLY2P9",
        "outputId": "3975bb69-a301-4830-98eb-a92e9df3a221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=986.91 +/- 493.98\n",
            "Episode length: 285.20 +/- 115.96\n",
            "Eval num_timesteps=1700000, episode_reward=2631.03 +/- 1235.79\n",
            "Episode length: 678.20 +/- 298.80\n",
            "Eval num_timesteps=1800000, episode_reward=2793.55 +/- 1337.50\n",
            "Episode length: 680.80 +/- 308.18\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1900000, episode_reward=1376.64 +/- 1255.50\n",
            "Episode length: 388.40 +/- 312.59\n",
            "Eval num_timesteps=2000000, episode_reward=2601.54 +/- 1124.36\n",
            "Episode length: 639.80 +/- 243.12\n",
            "Training time: 4025.4372096061707\n",
            "Test reward (avg +/- std): (1881.1000667336657 +/- 993.4588977343566) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on SOURCE environment WITHOUT Uniform Domain Randomization (UDR) and SEED = 42\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --seed=42 --model_id=\"walker2d_source_seed42\" --total_timesteps=2000000 --lr_schedule=\"exponential\"\n",
        "\n",
        "# Training on SOURCE environment WITHOUT Uniform Domain Randomization (UDR) and SEED = 123\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --seed=123 --model_id=\"walker2d_source_seed123\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kTYrMSe1NbNj",
        "outputId": "b78eec17-67ef-4ef8-9161-f651317f3a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=2872.74 +/- 1189.07\n",
            "Episode length: 745.80 +/- 284.63\n",
            "Eval num_timesteps=1700000, episode_reward=3648.63 +/- 1024.54\n",
            "Episode length: 882.60 +/- 234.80\n",
            "Eval num_timesteps=1800000, episode_reward=3055.46 +/- 1586.04\n",
            "Episode length: 720.00 +/- 344.21\n",
            "Eval num_timesteps=1900000, episode_reward=3631.84 +/- 1235.82\n",
            "Episode length: 864.20 +/- 271.60\n",
            "Eval num_timesteps=2000000, episode_reward=3836.39 +/- 1078.95\n",
            "Episode length: 881.60 +/- 236.80\n",
            "Training time: 3665.834237098694\n",
            "Test reward (avg +/- std): (3386.996509596658 +/- 1294.4795078592426) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on SOURCE environment with Uniform Domain Randomization (UDR) and SEED = 42\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --domain_rand=uniform --seed=42 --model_id=\"walker2d_source_udr_seed42\" --total_timesteps=2000000 --lr_schedule=\"exponential\"\n",
        "\n",
        "# Training on TARGET environment with SEED = 42\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-target-v0 --seed=42 --model_id=\"walker2d_target_udr_seed42\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JorB8RJIIaBt",
        "outputId": "92bd8f59-1f44-43b9-843a-70f6be537306"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=1600000, episode_reward=986.91 +/- 493.98\n",
            "Episode length: 285.20 +/- 115.96\n",
            "Eval num_timesteps=1700000, episode_reward=2631.03 +/- 1235.79\n",
            "Episode length: 678.20 +/- 298.80\n",
            "Eval num_timesteps=1800000, episode_reward=2793.55 +/- 1337.50\n",
            "Episode length: 680.80 +/- 308.18\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1900000, episode_reward=1376.64 +/- 1255.50\n",
            "Episode length: 388.40 +/- 312.59\n",
            "Eval num_timesteps=2000000, episode_reward=2601.54 +/- 1124.36\n",
            "Episode length: 639.80 +/- 243.12\n",
            "Training time: 3812.2031903266907\n",
            "Test reward (avg +/- std): (1881.1000667336657 +/- 993.4588977343566) - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Training on SOURCE environment with Uniform Domain Randomization (UDR) and SEED = 123\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-source-v0 --domain_rand=uniform --seed=123 --model_id=\"walker2d_source_udr_seed123\" --total_timesteps=2000000 --lr_schedule=\"exponential\"\n",
        "\n",
        "# Training on TARGET environment with SEED = 123\n",
        "!cd sim2real; python train.py --env=CustomWalker2D-target-v0 --seed=123 --model_id=\"walker2d_target_seed123\" --total_timesteps=2000000 --lr_schedule=\"exponential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MdYemi7iaPi9",
        "outputId": "a8da95bc-932f-4a1a-db8a-8520f27ae1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TESTING MODELS WITH SEED=42 WITHOUT UDR=====\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed42_no_udr_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (4171.7620666513085 +/- 729.9510470707787) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed42_no_udr_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (4222.225900392046 +/- 633.1598529827234) - Num episodes: 1000\n",
            "\n",
            "----- TARGET ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed42_no_udr_target_to_target_test.mp4\n",
            "Test reward (avg +/- std): (4202.76837320566 +/- 672.1045929516745) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n===== TESTING MODELS WITH SEED=42 WITHOUT UDR=====\")\n",
        "\n",
        "# SOURCE ➔ SOURCE\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_seed42\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed42_no_udr_source_to_source_test.mp4\"\n",
        "\n",
        "# SOURCE ➔ TARGET\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_seed42\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed42_no_udr_source_to_target_test.mp4\"\n",
        "\n",
        "# TARGET ➔ TARGET\n",
        "print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_target_seed42\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed42_no_udr_target_to_target_test.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jucm3YrlauYb",
        "outputId": "24884f1b-48d9-4f64-c0f5-2e2a3b23c67d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TESTING MODELS WITH SEED=123 WITHOUT UDR=====\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed123_no_udr_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (2409.717632176848 +/- 1119.5198288526162) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed123_no_udr_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (2450.2931949044223 +/- 1127.6418051425496) - Num episodes: 1000\n",
            "\n",
            "----- TARGET ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed123_no_udr_target_to_target_test.mp4\n",
            "Test reward (avg +/- std): (2420.6477335056934 +/- 1142.664378090421) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n===== TESTING MODELS WITH SEED=123 WITHOUT UDR=====\")\n",
        "\n",
        "# SOURCE ➔ SOURCE\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_seed123\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed123_no_udr_source_to_source_test.mp4\"\n",
        "\n",
        "# SOURCE ➔ TARGET\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_seed123\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed123_no_udr_source_to_target_test.mp4\"\n",
        "\n",
        "# TARGET ➔ TARGET\n",
        "print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_target_seed123\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed123_no_udr_target_to_target_test.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pnfc3eZjIaBt",
        "outputId": "6751496e-a713-4a1a-8220-65d85b53f910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TESTING MODELS WITH SEED=42 =====\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed42_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (1161.746603501175 +/- 137.62568608540383) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed42_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (1161.746603501175 +/- 137.62568608540383) - Num episodes: 1000\n",
            "\n",
            "----- TARGET ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed42_target_to_target_test.mp4\n",
            "Test reward (avg +/- std): (4205.180492831578 +/- 674.4769696711085) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n===== TESTING MODELS WITH SEED=42 AND UDR=====\")\n",
        "\n",
        "# SOURCE ➔ SOURCE\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_udr_seed42\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed42_source_to_source_test.mp4\"\n",
        "\n",
        "# SOURCE ➔ TARGET\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_udr_seed42\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed42_source_to_target_test.mp4\"\n",
        "\n",
        "# TARGET ➔ TARGET\n",
        "print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_target_seed42\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed42_target_to_target_test.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OZrrJtenIaBt",
        "outputId": "c6070337-bb7a-423b-8011-6f6ad084f2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TESTING MODELS WITH SEED=123 =====\n",
            "\n",
            "----- SOURCE ➔ SOURCE -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed123_source_to_source_test.mp4\n",
            "Test reward (avg +/- std): (3857.710568920987 +/- 110.33301119525349) - Num episodes: 1000\n",
            "\n",
            "----- SOURCE ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed123_source_to_target_test.mp4\n",
            "Test reward (avg +/- std): (3857.710568920987 +/- 110.33301119525349) - Num episodes: 1000\n",
            "\n",
            "----- TARGET ➔ TARGET -----\n",
            "Testing...\n",
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Test video save as videos/walker2d_seed123_target_to_target_test.mp4\n",
            "Test reward (avg +/- std): (2421.7487060438625 +/- 1135.601343373323) - Num episodes: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n===== TESTING MODELS WITH SEED=123 AND UDR =====\")\n",
        "\n",
        "# SOURCE ➔ SOURCE\n",
        "print(\"\\n----- SOURCE ➔ SOURCE -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_seed123\" --env=CustomWalker2D-source-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed123_source_to_source_test.mp4\"\n",
        "\n",
        "# SOURCE ➔ TARGET\n",
        "print(\"\\n----- SOURCE ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_source_seed123\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed123_source_to_target_test.mp4\"\n",
        "\n",
        "# TARGET ➔ TARGET\n",
        "print(\"\\n----- TARGET ➔ TARGET -----\")\n",
        "!cd sim2real; python train.py --test=\"models/PPO_model_walker2d_target_seed123\" --env=CustomWalker2D-target-v0 --test_episodes=1000 --test_rendering --video_name=\"walker2d_seed123_target_to_target_test.mp4\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
